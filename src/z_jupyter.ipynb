{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from config import client\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from config import column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      WORD|守る|ja  Word  ...  1706274142813_next.jpg  1706274142813.mp3\n",
      "0      WORD|子|ja  Word  ...  1706274141384_next.jpg  1706274141384.mp3\n",
      "1    WORD|代わり|ja  Word  ...  1706274138536_next.jpg  1706274138536.mp3\n",
      "2      WORD|事|ja  Word  ...  1706274135154_next.jpg  1706274135154.mp3\n",
      "3    WORD|生きる|ja  Word  ...  1706274132200_next.jpg  1706274132200.mp3\n",
      "4  WORD|タイミング|ja  Word  ...                     NaN  1706274128872.mp3\n",
      "\n",
      "[5 rows x 24 columns]\n",
      "(5, 24)\n"
     ]
    }
   ],
   "source": [
    "# load dataframe from csv\n",
    "df = pd.read_csv('../test_dataframes/jn_items.csv', delimiter='\\t', encoding='utf-8')\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "# load column_names from config.py\n",
    "df.columns = column_names\n",
    "\n",
    "dfa = df[['word', 'long_phrase', 'machine_translation']]\n",
    "\n",
    "# dfa to csv\n",
    "dfa.to_csv('dfa.csv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choices: [Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)]\n",
      "usage: None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChatCompletionChunk' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/src/LR2Anki/venv/lib/python3.12/site-packages/pydantic/main.py:759\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpydantic_extra\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'content'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;241m.\u001b[39mchoices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124musage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;241m.\u001b[39musage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m****************\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/src/LR2Anki/venv/lib/python3.12/site-packages/pydantic/main.py:761\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pydantic_extra[item]\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, item):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ChatCompletionChunk' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "# Example of an OpenAI ChatCompletion request with stream=True and stream_options={\"include_usage\": True}\n",
    "\n",
    "# a ChatCompletion request\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"Can you give me a summary of the pros and cons of the OpenAI API?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    "    # stream_options={\"include_usage\": True}, # retrieving token usage for stream response\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(f\"choices: {chunk.choices}\\nusage: {chunk.usage}\")\n",
    "    # print(f\"{chunk.content}\")\n",
    "    print(\"****************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content='Mor', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content='itz', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' could', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' refer', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' to', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' many', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' individuals', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' as', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' it', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' is', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' common', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' name', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' in', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' German', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content='-speaking', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' countries', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' Without', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' additional', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' context', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=\" it's\", function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' difficult', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' to', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' determine', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' exactly', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' which', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' Mor', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content='itz', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' are', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' referring', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' to', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' If', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' have', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' more', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' specific', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' information', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' about', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' Mor', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content='itz', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' such', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' as', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' his', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' last', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' name', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' profession', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' or', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' any', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' notable', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' achievements', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' could', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' provide', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' more', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' detailed', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=' answer', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9VIXMqyH2Vcb1bayBfZID39Crmnx7', choices=[], created=1717246452, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_319be4768e', usage=CompletionUsage(completion_tokens=67, prompt_tokens=24, total_tokens=91))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"Moritz is a great guy from Berlin\"},\n",
    "        # {'role': 'assistant', 'content': \"I don't know that\"},\n",
    "        {'role': 'user', 'content': \"who is Moritz?\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    "    stream_options={\"include_usage\": True}, # retrieving token usage for stream response\n",
    ")\n",
    "\n",
    "# print the content of the response\n",
    "\n",
    "# for chunk in stream:\n",
    "#     if chunk.choices[0].delta.content is not None:\n",
    "#         print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Full Response:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# List of messages to be sent to the chat completion endpoint\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Always answer briefly. My name is Moritz! What is your name?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Guess where I likely come from, based on my name\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please sum up this dialogue\"},\n",
    "]\n",
    "\n",
    "# Create the chat completion stream\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4\",  # Assuming you're using the latest GPT-4 model\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Initialize an empty response container\n",
    "full_response = \"\"\n",
    "\n",
    "# Iterate over the stream and print each chunk\n",
    "for chunk in stream:\n",
    "    if 'choices' in chunk:\n",
    "        delta = chunk['choices'][0]['delta']\n",
    "        if 'content' in delta:\n",
    "            full_response += delta['content']\n",
    "            print(delta['content'], end=\"\")\n",
    "\n",
    "# Optionally, print the full response at the end\n",
    "print(\"\\n\\nFull Response:\\n\", full_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object Stream can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(choice[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m], end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/LR2Anki/venv/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/src/LR2Anki/venv/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Create a streaming compion\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     16\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     18\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlways answer briefly. My name is Moritz! What is your name?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     19\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGuess where I likely come from, based on my name\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     20\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease sum up this dialogue\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     21\u001b[0m         ],\n\u001b[1;32m     22\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Print the streaming response\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "\u001b[0;31mTypeError\u001b[0m: object Stream can't be used in 'await' expression"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "client = OpenAI()\n",
    "# Apply the nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Create a streaming compion\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Always answer briefly. My name is Moritz! What is your name?\"},\n",
    "            {\"role\": \"user\", \"content\": \"Guess where I likely come from, based on my name\"},\n",
    "            {\"role\": \"user\", \"content\": \"Please sum up this dialogue\"}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # Print the streaming response\n",
    "    async for chunk in response:\n",
    "        if 'choices' in chunk:\n",
    "            choice = chunk['choices'][0]\n",
    "            if 'delta' in choice and 'content' in choice['delta']:\n",
    "                print(choice['delta']['content'], end=\"\")\n",
    "\n",
    "# Run the main function\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = Path(\"test_dataframes/italian_items/items_long.csv\")\n",
    "\n",
    "    # Load the csv file\n",
    "df = pd.read_csv(csv_file_path, delimiter='\\t')\n",
    "\n",
    "with open('config/column_names.yaml', 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "        column_names = data['column_names']\n",
    "\n",
    "\n",
    "df.columns = column_names\n",
    "\n",
    "# select the columns word and long_sentence\n",
    "\n",
    "df = df[['word', 'long_phrase']]\n",
    "df.to_csv('test_dataframes/italian_items/items_long_changed.csv', sep='\\t', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic Animal sound\n",
      "Generic Animal sound\n",
      "Woof!\n"
     ]
    }
   ],
   "source": [
    "class Animal:\n",
    "    def __init__(self):\n",
    "        self.animal_type = \"Generic Animal\"\n",
    "\n",
    "    def make_sound(self):\n",
    "        print(f\"{self.animal_type} sound\")\n",
    "\n",
    "class Dog:\n",
    "    def __init__(self):\n",
    "        self.animal_sound = \"Woof!\"\n",
    "        self.animal = Animal()  # Create an Animal instance\n",
    "\n",
    "    def make_sound(self):\n",
    "        self.animal.make_sound()  # Call base class method directly\n",
    "        print(self.animal_sound)\n",
    "\n",
    "# Create objects and call methods\n",
    "animal = Animal()\n",
    "dog = Dog()\n",
    "\n",
    "animal.make_sound()  # Output: Generic Animal sound\n",
    "dog.make_sound()  # Output: Generic Animal sound\n",
    "                #        Woof!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic Animal sound\n",
      "Dog sound\n",
      "Woof!\n"
     ]
    }
   ],
   "source": [
    "class Animal:\n",
    "    def __init__(self):\n",
    "        self.animal_type = \"Generic Animal\"\n",
    "\n",
    "    def make_sound(self):\n",
    "        print(f\"{self.animal_type} sound\")\n",
    "\n",
    "class Dog(Animal):\n",
    "    def make_sound(self):\n",
    "        self.animal_type = \"Dog\"  # Modify attribute before calling base class method\n",
    "        super().make_sound()\n",
    "        print(\"Woof!\")\n",
    "\n",
    "# Create objects and call methods\n",
    "animal = Animal()\n",
    "dog = Dog()\n",
    "\n",
    "animal.make_sound()  # Output: Generic Animal sound\n",
    "dog.make_sound()  # Output: Dog sound\n",
    "                #        Woof!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (3130921398.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    border: 1px solid #dddddd;\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Markdown Table Example</title>\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/marked@4.0.12/marked.min.js\"></script>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            margin: 40em;\n",
    "        }\n",
    "        table {\n",
    "            width: 50%;\n",
    "            border-collapse: collapse;\n",
    "        }\n",
    "        th, td {\n",
    "            border: 1px solid #dddddd;\n",
    "            text-align: left;\n",
    "            padding: 8px;\n",
    "        }\n",
    "        th {\n",
    "            background-color: #f2f2f2;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"markdown-content\"></div>\n",
    "\n",
    "    <script>\n",
    "        const markdownTable = `\n",
    "            | Name   | Age | Occupation   |\n",
    "            |--------|-----|--------------|\n",
    "            | John   | 25  | Engineer     |\n",
    "            | Alice  | 30  | Designer     |\n",
    "            | Bob    | 22  | Developer    |\n",
    "            `;\n",
    "\n",
    "        document.getElementById('markdown-content').innerHTML = marked.parse(markdownTable);\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
