{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-19_14-08\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# TODO write a part the processes the audio images and cloze using regex. \n",
       "\n",
       "# IDENTITY and PURPOSE\n",
       "\n",
       "You are a professional language teacher.. Your task is to provide concise, relevant information for sentence-word Anki flashcards, ensuring the student can effectively study vocabulary.\n",
       "\n",
       "# TOOLS\n",
       "\n",
       "You rely mainly on your capability as an LLM to predict the next string of characters. You don't need to analyze the table or anything.\n",
       "\n",
       "# INPUT:\n",
       "\n",
       "You will be given a csv containing sentence-word pairs from the Google extension 'LanguageReactor', containing the following columns:\n",
       "\n",
       "- 'Word'\n",
       "- 'Context'\n",
       "- 'Context machine translation'\n",
       "- ('Context human translation')\n",
       "\n",
       "The 'Context' column contains a sentence in Spanish, the target language. The 'Word' column contains one word that appears in the 'Context' sentence.\n",
       "\n",
       "! The delimiter in the csv is probably tab!\n",
       "\n",
       "# Steps\n",
       "\n",
       "1. ### Clean the data and check for parsing errors\n",
       "\n",
       "Have a look at the table I provided you with. Don't use code for that, just rely on your prediction of characters as LLM.\n",
       "\n",
       "- Remove unnecessary characters and correct weird formatting from 'Word' and 'Context'. However, pay attention that 'Word' always appears in 'Context'.\n",
       "- Check each row to ensure the 'Context' sentence is correctly parsed. The 'Word' should include the entire vocabulary word, not just a fragment. Sometimes parsers may miss the whole verb or expression. Also, check the 'Context machine translation' to see if the 'Word' makes sense in its 'Context'. If there is a parsing error and 'Word' is incomplete, adjust 'Word' to match the vocabulary in 'Context'. Ensure 'Word' is formatted exactly as it appears in 'Context' (including capitalization, grammar, punctuation, and spelling errors if present).\n",
       "\n",
       "2. ### Generate flashcard information\n",
       "\n",
       "   To assist the student, generate a table containing following information for each row:\n",
       "\n",
       "\n",
       "   1. Two or more synonyms for 'Word' based on its 'Context'.\n",
       "   2. Two or more translations for 'Word' based on its 'Context'.\n",
       "   3. A simple 'Example sentence' using 'Word'.\n",
       "   4. The translation of the 'Example sentence'.\n",
       "   5. A brief explanation of 'Word' in its 'Context'.\n",
       "   6. A short explanation of the grammar\n",
       "\n",
       "   When generating this information, stick to the following principles:\n",
       "\n",
       "   - Minimum Information Principle: Formulate the material in the simplest possible way without losing essential information. That means you can safely omit conjunctions like 'or', 'and' and you don't need to say: 現実 means 'reality' or 'actuality'. Instead just say: 現実: reality, actuality.\n",
       "   - Optimize Wording: Ensure the wording is precise and efficient to trigger the correct response quickly.\n",
       "  \n",
       "  The table should contain 7 columns with following column names:\n",
       "   - 'Word'\n",
       "   - 'Context'\n",
       "   - 'Synonyms'\n",
       "   - 'Translations'\n",
       "   - 'Example'\n",
       "   - 'Example translation'\n",
       "   - 'Explanation'\n",
       "\n",
       "\n",
       "# Output\n",
       "\n",
       "# TODO output should be csv so that gpt can\n",
       "\n",
       "Output the generated information as a Markdown table, including the column names as headers.  \n",
       "- Do not include warnings or notes in the output—only the requested sections.\n",
       "- Do not include additional information like 'here is the markdown table' or anything else. The only thing I want is the markdown table.\n",
       "\n",
       "404: Not Found\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "from config import column_names\n",
    "\n",
    "# load markdown from github repository https://github.com/moritzvitt/moritzProjekt/blob/markdown/system_japanese.md\n",
    "# class BaseAnkiDeckGenerator:\n",
    "#     def __init__(self, df: pd.DataFrame):\n",
    "#         self.ai_input_df = self.df[['short_phrase', 'short_translation', 'word']]\n",
    "\n",
    "\n",
    "target_language = \"ger\"\n",
    "    # @log_io\n",
    "\n",
    "def load_markdown():\n",
    "    url1 = \"https://raw.githubusercontent.com/moritzvitt/moritzProjekt/markdown/prompts/_general_prompt.md\"\n",
    "\n",
    "    examples_url = f\"https://raw.githubusercontent.com/moritzvitt/moritzProjekt/markdown/prompts/{target_language}_examples.md\"\n",
    "    # furigana_url = f\"https://raw.githubusercontent.com/moritzvitt/moritzProjekt/markdown/prompts/{target_language}_add_furigana.md\"\n",
    "    url2 = examples_url\n",
    "    # url3 = furigana_url\n",
    "    # url2 = \"https://raw.githubusercontent.com/moritzvitt/moritzProjekt/markdown/add_furigana.md\"\n",
    "    \n",
    "    response1 = requests.get(url1)\n",
    "    markdown1 = response1.text\n",
    "\n",
    "    response2 = requests.get(url2)\n",
    "    markdown2 = response2.text\n",
    "\n",
    "    # response3 = requests.get(url3)\n",
    "    # markdown3 = response3.text\n",
    "\n",
    "    merged_markdown = markdown1 + \"\\n\" + markdown2 + \"\\n\" \n",
    "    # + markdown3\n",
    "    return merged_markdown\n",
    "\n",
    "# load csv from test_dataframes \n",
    "\n",
    "# load dataframe from csv\n",
    "df = pd.read_csv('../test_dataframes/japanese_items/items.csv', delimiter='\\t', encoding='utf-8')\n",
    "# print(df.head())\n",
    "# print(df.shape)\n",
    "\n",
    "# load column_names from config.py\n",
    "df.columns = column_names\n",
    "\n",
    "df = df[[\n",
    "        \"Word\", \n",
    "        \"Context\",\n",
    "        \"Context machine translation\",\n",
    "        \"Context human translation\",\n",
    "        ]]\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "print(formatted_time)\n",
    "\n",
    "# run \n",
    "def run():\n",
    "    markdown = load_markdown()\n",
    "    display(Markdown(markdown))\n",
    "    # also safe the markdown to a new file\n",
    "    # create final.md name with random number at the end\n",
    "    # add the date in an f string\n",
    "\n",
    "\n",
    "\n",
    "    with open(f\"final{target_language}{formatted_time}.md\", \"w\") as file:\n",
    "        file.write(markdown+'\\n'+'\\nThis is the table with the word sentence pairs: \\n\\n'+df.to_csv(sep='\\t', encoding='utf-8', index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    WORD|胡椒|ja  Word        最後に塩と胡椒で味を整えてください  \\\n",
      "0   WORD|程度|ja  Word              最小限つなぎ程度にして   \n",
      "1  WORD|最小限|ja  Word              最小限つなぎ程度にして   \n",
      "2   WORD|刻む|ja  Word         まずゆで卵を適度に刻んでください   \n",
      "3   WORD|適度|ja  Word         まずゆで卵を適度に刻んでください   \n",
      "4  WORD|さすが|ja  Word  さすがにまだ卵サンドは食べられないみたいだけど   \n",
      "\n",
      "                  Finally, add some salt and pepper.   胡椒 胡椒.1  Noun  \\\n",
      "0  We're going to make sure that we have the leas...   程度   程度  Noun   \n",
      "1  We're going to make sure that we have the leas...  最小限  最小限  Noun   \n",
      "2  First of all, you need to mark the eggs in mod...   刻ん   刻む  Verb   \n",
      "3  First of all, you need to mark the eggs in mod...   適度   適度  Noun   \n",
      "4           I can't seem to eat an egg sandwich yet.  さすが  さすが   Adv   \n",
      "\n",
      "   Unnamed: 7                         pepper, black pepper  Netflix  ...  216  \\\n",
      "0         NaN                       degree, extent, amount  Netflix  ...  215   \n",
      "1         NaN  minimum, minimum limit, minimum requirement  Netflix  ...  215   \n",
      "2         NaN                      carved, engrave, minced  Netflix  ...  212   \n",
      "3         NaN              moderate, appropriate, suitable  Netflix  ...  212   \n",
      "4         NaN       As expected, indeed, Just as I thought  Netflix  ...  211   \n",
      "\n",
      "   80113548  Midnight Diner E7 Episode 7  2024-05-17 15:11  \\\n",
      "0  80113548  Midnight Diner E7 Episode 7  2024-05-17 15:10   \n",
      "1  80113548  Midnight Diner E7 Episode 7  2024-05-17 15:10   \n",
      "2  80113548  Midnight Diner E7 Episode 7  2024-05-17 15:10   \n",
      "3  80113548  Midnight Diner E7 Episode 7  2024-05-17 15:10   \n",
      "4  80113548  Midnight Diner E7 Episode 7  2024-05-17 15:09   \n",
      "\n",
      "      最小限つなぎ程度にして\\n最後に塩と胡椒で味を整えてください\\nあ まずあの 僕あのこっちで  \\\n",
      "0  ただし入れすぎると卵の味よりマヨネーズの味が勝ってしまうので\\n最小限つなぎ程度にして\\n最...   \n",
      "1  ただし入れすぎると卵の味よりマヨネーズの味が勝ってしまうので\\n最小限つなぎ程度にして\\n最...   \n",
      "2  さすがにまだ卵サンドは食べられないみたいだけど\\nまずゆで卵を適度に刻んでください\\nマヨネ...   \n",
      "3  さすがにまだ卵サンドは食べられないみたいだけど\\nまずゆで卵を適度に刻んでください\\nマヨネ...   \n",
      "4  その後も毎日中島くんは新聞を配っている\\nさすがにまだ卵サンドは食べられないみたいだけど\\n...   \n",
      "\n",
      "   We're going to make sure that we have the least amount of connections.\\nFinally, add some salt and pepper.\\nOh, first of all, I'm over there.  \\\n",
      "0  But if you put too much in it, it tastes like ...                                                                                               \n",
      "1  But if you put too much in it, it tastes like ...                                                                                               \n",
      "2  I can't seem to eat an egg sandwich yet.\\nFirs...                                                                                               \n",
      "3  I can't seem to eat an egg sandwich yet.\\nFirs...                                                                                               \n",
      "4  Ever since then, I've been delivering newspape...                                                                                               \n",
      "\n",
      "  Keep the mayonnaise to a minimum,\\nand add salt and pepper to round out the flavors.\\nMaster. I brought my own bread.  \\\n",
      "0  But don't go overboard with it, or the mayonna...                                                                      \n",
      "1  But don't go overboard with it, or the mayonna...                                                                      \n",
      "2  But he hasn't had egg sandwiches since then.\\n...                                                                      \n",
      "3  But he hasn't had egg sandwiches since then.\\n...                                                                      \n",
      "4  Nakajima still delivers the papers every day.\\...                                                                      \n",
      "\n",
      "   1715958665910_prev.jpg  1715958665910_next.jpg  1715958665910.mp3  \n",
      "0  1715958659411_prev.jpg  1715958659411_next.jpg  1715958659411.mp3  \n",
      "1  1715958656851_prev.jpg  1715958656851_next.jpg  1715958656851.mp3  \n",
      "2  1715958621198_prev.jpg  1715958621198_next.jpg  1715958621198.mp3  \n",
      "3  1715958619583_prev.jpg  1715958619583_next.jpg  1715958619583.mp3  \n",
      "4  1715958582029_prev.jpg  1715958582029_next.jpg  1715958582029.mp3  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "(419, 24)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "from config import column_names\n",
    "\n",
    "# Base class for common functionalities\n",
    "class BaseAnkiDeckGenerator:\n",
    "    def __init__(self, csv_path: str, delimiter: str = '\\t', encoding: str = 'utf-8'):\n",
    "        self.df = pd.read_csv(csv_path, delimiter=delimiter, encoding=encoding)\n",
    "        self.df.columns = column_names\n",
    "        self.df = self.df[[\n",
    "            \"Word\", \n",
    "            \"Context\",\n",
    "            \"Context machine translation\",\n",
    "            \"Context human translation\",\n",
    "        ]]\n",
    "        \n",
    "    def display_markdown(self, markdown: str):\n",
    "        display(Markdown(markdown))\n",
    "        \n",
    "    def save_markdown(self, markdown: str, file_name: str = \"final.md\"):\n",
    "        with open(file_name, \"w\") as file:\n",
    "            file.write(markdown + '\\n\\nThis is the table with the word sentence pairs:\\n\\n' + self.df.to_csv(sep='\\t', encoding='utf-8', index=False))\n",
    "\n",
    "\n",
    "# Japanese specific AnkiDeckGe nerator\n",
    "class JapaneseAnkiDeckGenerator(BaseAnkiDeckGenerator):\n",
    "    def __init__(self, csv_path: str):\n",
    "        super().__init__(csv_path)\n",
    "        self.markdown_urls = [\n",
    "            \"https://raw.githubusercontent.com/moritzvitt/moritzProjekt/markdown/prompts/_general_prompt.md\",\n",
    "            \"https://raw.githubusercontent.com/moritzvitt/moritzProjekt/markdown/prompts/jn_examples.md\"\n",
    "        ]\n",
    "        \n",
    "    def load_markdown(self):\n",
    "        markdowns = [requests.get(url).text for url in self.markdown_urls]\n",
    "        return \"\\n\".join(markdowns)\n",
    "\n",
    "# Define other language-specific classes similarly if needed\n",
    "# class FrenchAnkiDeckGenerator(BaseAnkiDeckGenerator):\n",
    "#     ...\n",
    "\n",
    "# Run the process\n",
    "def run():\n",
    "    # Path to the Japanese CSV file\n",
    "    csv_path = '../test_dataframes/japanese_items/items.csv'\n",
    "    japanese_generator = JapaneseAnkiDeckGenerator(csv_path)\n",
    "    \n",
    "    markdown = japanese_generator.load_markdown()\n",
    "    japanese_generator.display_markdown(markdown)\n",
    "    japanese_generator.save_markdown(markdown)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "@log_io\n",
    "def generate_anki_deck(df: pd.DataFrame) -> genanki.Package:\n",
    "    \"\"\"Generates an Anki deck from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing card data.\n",
    "\n",
    "    Returns:\n",
    "        genanki.Package: The generated Anki package.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('templates/anki_card.html', 'r', encoding='utf-8') as content_file:\n",
    "        content = content_file.read()\n",
    "\n",
    "    # Splitting HTML content\n",
    "    html_sections = content.split('<!-- html -->')\n",
    "\n",
    "    # Assigning sections to qfmt, afmt, and css\n",
    "    qfmt_html = html_sections[1]\n",
    "    afmt_html = html_sections[2]\n",
    "\n",
    "    with open('static/css/anki_card.css', 'r', encoding='utf-8') as content_file:\n",
    "        css_code = content_file.read()\n",
    "\n",
    "    # Ensure all columns are strings\n",
    "    df = df.astype(str)\n",
    "\n",
    "    # Define the Anki model\n",
    "    model_id = 1607392319\n",
    "    model = genanki.Model(\n",
    "        model_id,\n",
    "        'Language Learning with Netflix Model',\n",
    "        fields = fields_config[\"fields\"],\n",
    "        templates=[\n",
    "            {\n",
    "                'name': 'Card 1',\n",
    "                'qfmt': qfmt_html,\n",
    "                'afmt': afmt_html,\n",
    "            },\n",
    "        ],\n",
    "        css=css_code\n",
    "    )\n",
    "\n",
    "    # Create an Anki deck\n",
    "    deck_id = model_id + 1  # Ensure deck_id is different from model_id\n",
    "    deck = genanki.Deck(deck_id, 'lln_anki_deck')\n",
    "\n",
    "    # Add cards to the deck\n",
    "    for index, row in df.iterrows():\n",
    "        my_note = genanki.Note(\n",
    "            model=model,\n",
    "            fields=[row['ID'], row['cloze'], row['hint'], row['definition'], row['notes'], row['image'], row['audio']],\n",
    "        )\n",
    "        deck.add_note(my_note)\n",
    "\n",
    "    apkg_package = genanki.Package(deck)\n",
    "    return apkg_package\n",
    "\n",
    "@log_io\n",
    "def export_df(df: pd.DataFrame, package: genanki.Package, native_language: str, output_file_path: str, encoding: str = 'utf-8') -> Tuple[str, str]:\n",
    "    \"\"\"Exports an Anki package and a cleaned DataFrame to CSV.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to export.\n",
    "        package (genanki.Package): The Anki package to save.\n",
    "        native_language (str): The native language of the data.\n",
    "        output_file_path (str): The path to save the files.\n",
    "        encoding (str, optional): The encoding for the CSV file. Defaults to 'utf-8'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: A tuple containing the paths to the exported Anki package and CSV file.\n",
    "    \"\"\"\n",
    "    current_time = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "    package_path = os.path.join(output_file_path, f'{native_language}_LLN_{current_time}.apkg')\n",
    "    package.write_to_file(package_path)\n",
    "\n",
    "    csv_file_path = os.path.join(output_file_path, f'{native_language}_LLN_{current_time}.csv')\n",
    "    df.to_csv(csv_file_path, index=False, sep='\\t', encoding=encoding)\n",
    "\n",
    "    return package_path, csv_file_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
